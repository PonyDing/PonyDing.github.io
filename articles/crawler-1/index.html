<!DOCTYPE html>
<html lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="description" content="Keep learning.">


    <meta name="keywords" content="Python,Distributed Computing,AI">


<title>网络爬虫学习笔记（一）| 基本库urllib的使用 | DingZhi&#39;s Blog</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
    


</head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">DingZhi&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>

        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">DingZhi&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?3c2cb6a51b306a50c53ae432e39de395";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">网络爬虫学习笔记（一）| 基本库urllib的使用</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">Ding Zhi</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">September 3, 2019&nbsp;&nbsp;6:05:03</a>
                        </span>
                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/Python/">Python</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <blockquote class="pullquote left"><p><strong><font size="6">前言 </font></strong>最近学习了基于Python的网络爬虫，都说写博客是对自己所学知识进行再次总结与梳理的很好的方式，因此在完成了学习后，我希望能在日常学习之余，找出一部分时间再来回顾一下我的Python爬虫学习之路，既能进一步巩固的爬虫知识，又能让我对Python相关知识体系有更清晰的认识。下面就让我们开始Python网络爬虫的学习笔记专栏吧.</p>
</blockquote>

<p>我的Python爬虫学习路线是这样的：先学习Python网络爬虫的相关知识，并在随后进一步学习基于Python的数据分析，最后根据PyQt库制作出一个具有较好的UI设计成熟程序。我们的学习之路将从Python网络爬虫的基本知识开始。众所周知，编程技能的学习更多的是依赖实际的代码训练项目，而非简单的阅读教材与观看视频，所以我也会将我在更新Python爬虫笔记过程中写的代码在我的GitHub上同步更新，希望大家多多关注和支持。</p>
<p>学习爬虫，首先需要掌握一系列基本的库的使用，在今天的这篇文章里我们将一同学习urllib这个基本的库。</p>
<p>urllib是Python内置的http请求库，它有下面四个模块：</p>
<ul>
<li><code>request</code>：模拟发送请求，通过给库方法传入url以及其它额外的参数，就可以模拟这个过程</li>
<li><code>error</code>：异常处理模块，当出现请求错误时，利用此模块捕获异常并重新尝试或者进行其他操作保证程序不会出现异常</li>
<li><code>parse</code>：提供了拆分、解析、合并等URL处理方法</li>
<li><code>robotparser</code>：用来识别网站<code>robots.txt</code>文件</li>
</ul>
<p>下面我们将分别介绍这几个模块</p>
<h3 id="request模块——发送请求并得到响应"><a href="#request模块——发送请求并得到响应" class="headerlink" title="request模块——发送请求并得到响应"></a><code>request</code>模块——发送请求并得到响应</h3><h4 id="urlopen"><a href="#urlopen" class="headerlink" title="urlopen()"></a><code>urlopen()</code></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#urlopen()函数原型</span></span><br><span class="line">urllib.request.urlopen(url,data=<span class="literal">None</span>,[timeout,]*,cafile=<span class="literal">None</span>,capath=<span class="literal">None</span>,cadefault=<span class="literal">False</span>,context=<span class="literal">None</span>)</span><br><span class="line"><span class="comment">#data为附加数据，timeout为超时时间</span></span><br></pre></td></tr></table></figure>

<p>这是一种最基本的构造HTTP请求的方法，可以用来模拟浏览器的一个请求发起过程，还可以处理授权验证、重定向、浏览器Cookies和其他内容.利用此方法可以实现最基本的简单网页GET请求抓取。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">response=urllib.request.urlopen(<span class="string">'http://www.dingzhi.ga'</span>)	<span class="comment">#定义HTTPResponse类型对象response</span></span><br><span class="line">print(response.read().decode(<span class="string">'utf-8'</span>))	<span class="comment">#使用HTTPResponse类型对象的read()方法并利用decode()方法</span></span><br><span class="line">print(type(response))	<span class="comment">#返回response的类型HTTPResponse</span></span><br><span class="line">print(response.status)	<span class="comment">#返回response的状态码，此处为200，代表请求成功，例如404代表网页未找到</span></span><br><span class="line">print(response.getheaders())	<span class="comment">#返回response的响应头信息</span></span><br><span class="line">print(response.getheader(<span class="string">'Server'</span>))	<span class="comment">#返回response的响应头信息中Server项</span></span><br></pre></td></tr></table></figure>

<p>下面是<code>urlopen()</code>方法的几个其他可选参数的使用方法.</p>
<h5 id="data参数"><a href="#data参数" class="headerlink" title="data参数"></a>data参数</h5><p>如果添加该参数，并且它是字节流编码格式的内容，即<code>bytes</code>类型，则需要<code>bytes()</code>方法转化，另外，如果传递了这个参数，那么请求方式就不再是GET方式，而是POST方式。下面是一个示例代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">data = bytes(urllib.parse.urlencode(&#123;<span class="string">'word'</span>: <span class="string">'hello'</span>&#125;), encoding=<span class="string">'utf8'</span>)	<span class="comment">#使用bytes()方法转字节流，该方法需要的第一个参数是str类型，需要使用urllib.parse.urlencode()方法将参数字典转为字符串；第二个参数指定编码格式，这里定为utf8</span></span><br><span class="line">response = urllib.request.urlopen(<span class="string">'http://httpbin.org/post'</span>, data=data)	<span class="comment">#传递了参数word，值为hello，它需要被转码成bytes类型</span></span><br><span class="line">print(response.read())</span><br></pre></td></tr></table></figure>

<p>运行上述例程后可以在运行结果中看到我们传递的参数，证明这模拟了表单提交方式，以POST方式传输数据。</p>
<h5 id="timeout参数"><a href="#timeout参数" class="headerlink" title="timeout参数"></a>timeout参数</h5><p>设置超时时间，单位为秒。如果请求超出了设置的这个时间仍然没有响应就会抛出异常。下面是一段示例程序：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">response = urllib.request.urlopen(<span class="string">'http://httpbin.org/get'</span>, timeout=<span class="number">1</span>)</span><br><span class="line">print(response.read())</span><br></pre></td></tr></table></figure>

<p>运行时系统抛出了URLError异常，错误原因是超时.</p>
<blockquote>
<p>在实际应用中，这个参数可以用来控制一个网页如果长时间没有响应，就跳过爬取这个网站，可以通过try-except语句实现，也就是实现超时处理。相关代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="keyword">import</span> socket</span><br><span class="line">&gt; <span class="keyword">import</span> urllib.request</span><br><span class="line">&gt; <span class="keyword">import</span> urllib.error</span><br><span class="line">&gt; <span class="keyword">try</span>:</span><br><span class="line">&gt;     response = urllib.request.urlopen(<span class="string">'http://httpbin.org/get'</span>, timeout=<span class="number">0.1</span>)</span><br><span class="line">&gt; <span class="keyword">except</span> urllib.error.URLError <span class="keyword">as</span> e:</span><br><span class="line">&gt;     <span class="keyword">if</span> isinstance(e.reason, socket.timeout):</span><br><span class="line">&gt;         print(<span class="string">'TIME OUT'</span>)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p>上面的示例程序在最终输出了TIME OUT。这在实际应用中有时很有用处。</p>
</blockquote>
<h4 id="Request"><a href="#Request" class="headerlink" title="Request"></a><code>Request</code></h4><p>利用<code>Request</code>可以实现在请求中加入<code>Headers</code>等信息的需求。构造方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">urllib</span>.<span class="title">request</span>.<span class="title">Request</span><span class="params">(url,data=NNone,headers=&#123;&#125;,origin_req_host=None,unverifiable=False,method=None)</span></span></span><br><span class="line"><span class="class">#参数说明：</span></span><br><span class="line"><span class="class">#<span class="title">data</span>:</span>必须传入bytes类型，如果它是字典，需要先使用urllib.parse.urlencode()进行编码</span><br><span class="line"><span class="comment">#headers:请求头是字典类型，可以通过headers参数直接构造请求，也可以利用add_header()方法添加，这个参数常用来在爬取时应对部分网站反爬虫措施时伪装浏览器，下面将会详细展开</span></span><br><span class="line"><span class="comment">#origin_req_host:请求放host名称或IP</span></span><br><span class="line"><span class="comment">#unverifiable:这个请求是否无法验证</span></span><br><span class="line"><span class="comment">#method:指示请求使用的方法，如POST、GET和PUT等</span></span><br></pre></td></tr></table></figure>

<p>下面我们来实际练习一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, parse</span><br><span class="line">url = <span class="string">'http://httpbin.org/post'</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'</span>,</span><br><span class="line">    <span class="string">'Host'</span>: <span class="string">'httpbin.org'</span></span><br><span class="line">&#125;</span><br><span class="line">dict = &#123;</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'Germey'</span></span><br><span class="line">&#125;</span><br><span class="line">data = bytes(parse.urlencode(dict), encoding=<span class="string">'utf8'</span>)</span><br><span class="line">req = request.Request(url=url, data=data, headers=headers, method=<span class="string">'POST'</span>)</span><br><span class="line">response = request.urlopen(req)</span><br><span class="line">print(response.read().decode(<span class="string">'utf-8'</span>))</span><br></pre></td></tr></table></figure>

<p>观察运行结果可以发现我们成功设置了data、headers、method，另外我们也可以用add_header()方法添加headers,并方便的构造请求：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">req = request.Request(url=url, data=data, method=<span class="string">'POST'</span>)</span><br><span class="line">req.add_header(<span class="string">'User-Agent'</span>, <span class="string">'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'</span>)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>我们常常利用爬虫模拟浏览器上网的方式来实现绕过很多网站反爬虫机制进行网页爬取，下面我们来详细介绍相关内容。这个方法的核心思想是利用Request伪装headers头文件，进而将爬虫伪装成一个浏览器。<br>而获取正常浏览器浏览时的headers的方法很简单：首先打开浏览器审查元素界面，点击Network选项卡</p>
<p><img src="https://images2017.cnblogs.com/blog/1047463/201801/1047463-20180124104313615-1628918194.png" alt="img"></p>
<p>接下来刷新网页：</p>
<p><img src="/articles/crawler-1/1567426658777.png" alt="1567426658777"></p>
<p>点击第一项dingzhi.ga后右边会显示Headers选项如下所示：</p>
<p><img src="/articles/crawler-1/1567426699516.png" alt="1567426699516"></p>
<p>在最后面就有User-Agent的信息，我们即可直接复制这段信息进入我们的代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line">&gt; url = <span class="string">"http://www.dingzhi.ga"</span></span><br><span class="line">&gt; header = &#123;<span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.108 Safari/537.36'</span>&#125;</span><br><span class="line">&gt; req = request.Request(url, headers = header)</span><br><span class="line">&gt; text = request.urlopen(req).read().decode()</span><br><span class="line">&gt; print(text)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p>这样就能成功的爬取到那些采取了反爬虫措施的网页信息了。</p>
</blockquote>
<h4 id="高级操作——Opener"><a href="#高级操作——Opener" class="headerlink" title="高级操作——Opener"></a>高级操作——Opener</h4><p>Opener可以使用<code>open()</code>方法，返回类型和<code>urlopen()</code>如出一辙，我们可以利用<code>Handler</code>构建<code>Opener</code>，实现高级功能（如验证、代理、Cookies等），此处我们略去不表，在未来用到时再做展开。</p>
<h3 id="error模块——处理异常"><a href="#error模块——处理异常" class="headerlink" title="error模块——处理异常"></a><code>error</code>模块——处理异常</h3><p><code>error</code>模块定义了由<code>request</code>模块产生的异常，如果出了问题，<code>request</code>模块就会抛出<code>error</code>模块定义的异常.</p>
<h4 id="URLError类"><a href="#URLError类" class="headerlink" title="URLError类"></a><code>URLError</code>类</h4><p>这个类来自<code>error</code>模块，继承自<code>OSError</code>类，是<code>error</code>异常模块的基类，<code>request</code>模块生成的异常都可以通过捕获这个类来处理，它具有一个属性<code>reason</code>，即返回错误的原因.下面是一个实例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, error</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">'http://www.dingzhi.ga/index.php'</span>)</span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure>

<p>我们打开一个不存在的页面，照理来说应该报错，但是由于捕获了<code>URLError</code>这个异常，运行后显示<code>Not Found</code>.这样我们就可以避免程序异常终止，并且在发生异常时进行有效处理.</p>
<h4 id="HTTPError类"><a href="#HTTPError类" class="headerlink" title="HTTPError类"></a><code>HTTPError</code>类</h4><p>是<code>URLError</code>的子类，专门用来处理HTTP请求错误，具有如下三个属性：</p>
<ul>
<li><code>code</code>：返回HTTP状态码，如404表示网站不存在，500表示服务器内部错误等</li>
<li><code>reason</code>：如同父类一样，用于返回错误的原因</li>
<li><code>headers</code>：返回请求头</li>
</ul>
<p>我们下面结合一个示例代码进行分析：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request,error</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">'http://www.dingzhi.ga/index.php'</span>)</span><br><span class="line"><span class="keyword">except</span> error.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason, e.code, e.headers, sep=<span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure>

<p>在上面的程序中由于捕获了<code>HTTPError</code>异常，因而正常输出了reason、code、headers等属性.</p>
<blockquote>
<p>考察到<code>URLError</code>是<code>HTTPError</code>的父类，所以可以先捕获子类的错误，再捕获父类的错误，上述代码在这种思路下可以进行优化，改写成为如下的代码，这也代表一种程序书写的范式与思路：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="keyword">from</span> urllib <span class="keyword">import</span> request, error</span><br><span class="line">&gt; <span class="keyword">try</span>:</span><br><span class="line">&gt;     response = request.urlopen(<span class="string">'http://www.dingzhi.ga/index.php'</span>)</span><br><span class="line">&gt; <span class="keyword">except</span> error.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">&gt;     print(e.reason, e.code, e.headers, sep=<span class="string">'\n'</span>)</span><br><span class="line">&gt; <span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">&gt;     print(e.reason)</span><br><span class="line">&gt; <span class="keyword">else</span>:</span><br><span class="line">&gt;     print(<span class="string">'Request Successfully'</span>)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>值得指出，有时reason属性返回的并不一定是字符串，也有可能是一个对象，下面我们看一段实例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.error</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = urllib.request.urlopen(<span class="string">'https://www.dingzhi.ga'</span>, timeout=<span class="number">0.001</span>)</span><br><span class="line"><span class="keyword">except</span> urllib.error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(type(e.reason))</span><br><span class="line">    <span class="keyword">if</span> isinstance(e.reason, socket.timeout):</span><br><span class="line">        print(<span class="string">'TIME OUT'</span>)</span><br></pre></td></tr></table></figure>

<p>通过输出结果我们可以看到，reason属性的结果是<code>socket.timeout</code>类，所以可以用<code>isinstance()</code>方法判断其类型，进而做出更详细的异常判断.</p>
<h3 id="parse模块——解析处理URL链接"><a href="#parse模块——解析处理URL链接" class="headerlink" title="parse模块——解析处理URL链接"></a><code>parse</code>模块——解析处理URL链接</h3><p>这个模块可以实现URL各部分的抽取、合并以及链接转换等，支持各种常见协议的URL处理，如ftp、http、https等，下面我们将介绍这个模块的常用方法.</p>
<h4 id="urlparse-——实现URL识别与分段"><a href="#urlparse-——实现URL识别与分段" class="headerlink" title="urlparse()——实现URL识别与分段"></a><code>urlparse()</code>——实现URL识别与分段</h4><p>我们通过考察下面一个实例的方式来分析这个方法的用途.</p>
<p>​<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line">result = urlparse(<span class="string">'http://www.baidu.com/index.html;user?id=5#comment'</span>)</span><br><span class="line">print(type(result), result)</span><br></pre></td></tr></table></figure></p>
<p>读者不妨自己在电脑上尝试运行这段代码，我们不难观察出，返回的结果为ParseResult类型的对象，包含scheme、netloc、path、params、query、fragment六个部分.结合我们输入的URL链接进行对照，为我们得到了以下的公式，事实上任何一个标准的URL都会符合这个规则，我们可以利用urlparse()方法将之进行拆分：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scheme://netloc/path;params?query#fragment</span><br></pre></td></tr></table></figure>

<p>下面我们考察<code>urlparse()</code>的API.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">urllib.parse.urlparse(urlstring,scheme=<span class="string">''</span>,allow_fragments=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li><p><code>urlstring</code>：必填项，即待解析URL</p>
</li>
<li><p><code>scheme</code>：默认的协议（如<code>http</code>或<code>https</code>）加入这个链接没有带协议信息，将会用这个作为默认协议，如下所示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line">result = urlparse(<span class="string">'www.baidu.com/index.html;user?id=5#comment'</span>, scheme=<span class="string">'https'</span>)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>

<p>从运行结果上看，scheme被指定为了https，但需要注意，当需要解析的链接带上了scheme时，最终返回的scheme以链接中的scheme为准，与API中传入的scheme无关，例如在下面的案例中，最终返回scheme为http而不是https:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line">result = urlparse(<span class="string">'http://www.baidu.com/index.html;user?id=5#comment'</span>, scheme=<span class="string">'https'</span>)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>allow_fragments</code>：即是否忽略fragment.如果被设置为False，fragment就会被自动忽略，它会被解析为path、parameters或者query中的一部分，而fragment部分为空.</p>
</li>
</ul>
<h4 id="urlunparse-——实现URL的整合与构造"><a href="#urlunparse-——实现URL的整合与构造" class="headerlink" title="urlunparse()——实现URL的整合与构造"></a><code>urlunparse()</code>——实现URL的整合与构造</h4><p>是前述方法的对立方法，我们直接通过下面的实例介绍其使用方法而不再过多文字说明：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlunparse</span><br><span class="line">data = [<span class="string">'http'</span>, <span class="string">'www.baidu.com'</span>, <span class="string">'index.html'</span>, <span class="string">'user'</span>, <span class="string">'a=5'</span>, <span class="string">'comment'</span>]</span><br><span class="line">print(urlunparse(data))</span><br><span class="line"><span class="comment">#运行结果为http://www.baidu.com/index.html;user?a=5#comment</span></span><br></pre></td></tr></table></figure>

<h4 id="urlsplit-——实现URL识别与分段"><a href="#urlsplit-——实现URL识别与分段" class="headerlink" title="urlsplit()——实现URL识别与分段"></a><code>urlsplit()</code>——实现URL识别与分段</h4><p>与<code>urlparse()</code>类似但是不解析<code>params</code>，只返回5个结果.返回结果是<code>SplitResult</code>类型，其实这是一个元组类型，既可以用属性获取值，也可以用索引获取值.</p>
<h4 id="urlunsplit-——实现URL的整合与构造"><a href="#urlunsplit-——实现URL的整合与构造" class="headerlink" title="urlunsplit()——实现URL的整合与构造"></a><code>urlunsplit()</code>——实现URL的整合与构造</h4><p>是前述方法的对立方法，我们直接通过下面的实例介绍其使用方法而不再过多文字说明：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlunsplit</span><br><span class="line">data = [<span class="string">'http'</span>, <span class="string">'www.baidu.com'</span>, <span class="string">'index.html'</span>, <span class="string">'a=6'</span>, <span class="string">'comment'</span>]</span><br><span class="line">print(urlunsplit(data))</span><br><span class="line"><span class="comment">#运行结果为http://www.baidu.com/index.html;user?a=6#comment</span></span><br></pre></td></tr></table></figure>

<h4 id="urljoin-——实现URL的解析、拼合与生成"><a href="#urljoin-——实现URL的解析、拼合与生成" class="headerlink" title="urljoin()——实现URL的解析、拼合与生成"></a><code>urljoin()</code>——实现URL的解析、拼合与生成</h4><p>利用这个方法，我们可以提供一个<code>base_url</code>作为第一个参数，将新的链接作为第二个参数，该方法会分析<code>base_url</code>的结构并对新链接缺失的部分进行补充并最终返回结果.下面是一个案例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urljoin</span><br><span class="line">print(urljoin(<span class="string">'http://www.baidu.com'</span>, <span class="string">'FAQ.html'</span>))</span><br><span class="line">print(urljoin(<span class="string">'http://www.baidu.com'</span>, <span class="string">'http://www.dingzhi.ga'</span>))</span><br><span class="line">print(urljoin(<span class="string">'http://www.baidu.com/about.html'</span>, <span class="string">'http://www.dingzhi.ga/index.html'</span>))</span><br><span class="line">print(urljoin(<span class="string">'http://www.baidu.com/about.html'</span>, <span class="string">'http://www.dingzhi.ga/articles/Python-DataScience-1/#%E5%9B%BE%E5%83%8F%E7%9A%84%E5%8F%98%E6%8D%A2'</span>))</span><br><span class="line">print(urljoin(<span class="string">'http://www.baidu.com?wd=abc'</span>, <span class="string">'http://www.dingzhi.ga/index.html'</span>))</span><br><span class="line">print(urljoin(<span class="string">'http://www.baidu.com'</span>, <span class="string">'?category=2#comment'</span>))</span><br><span class="line">print(urljoin(<span class="string">'www.baidu.com'</span>, <span class="string">'?category=2#comment'</span>))</span><br><span class="line">print(urljoin(<span class="string">'www.baidu.com#comment'</span>, <span class="string">'?category=2'</span>))</span><br></pre></td></tr></table></figure>

<p>可以发现，<code>base_url</code>提供了scheme、netloc、path三项内容，如果这三项内容在新的链接里面不存在就进行补充，如果新的链接里面存在就是用新的链接的部分，而<code>base_url</code>中params、query、fragment是不起作用的.</p>
<h4 id="urlencode-——实现GET请求参数的构造"><a href="#urlencode-——实现GET请求参数的构造" class="headerlink" title="urlencode()——实现GET请求参数的构造"></a><code>urlencode()</code>——实现GET请求参数的构造</h4><p>下面我们通过一段示例代码进行介绍:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'dingzhi'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">20</span></span><br><span class="line">&#125;	<span class="comment">#声明了一个字典将参数进行表示</span></span><br><span class="line">base_url = <span class="string">'http://www.baidu.com?'</span></span><br><span class="line">url = base_url + urlencode(params)	<span class="comment">#调用urlencode()方法将其序列化为GET请求参数</span></span><br><span class="line">print(url)</span><br><span class="line"><span class="comment">#运行结果:http://www.baidu.com?name=dingzhi&amp;age=20</span></span><br></pre></td></tr></table></figure>

<p>可以看到，参数成功的由字典类型转换为GET 请求参数了，这个方法非常常用，有时为了更加方便的构造参数，我们会事先用字典表示，要转化为URL的参数时，只需要调用该方法即可.</p>
<h4 id="parse-qs-——实现GET请求参数的反序列化"><a href="#parse-qs-——实现GET请求参数的反序列化" class="headerlink" title="parse_qs()——实现GET请求参数的反序列化"></a><code>parse_qs()</code>——实现GET请求参数的反序列化</h4><p>利用<code>parse_qs()</code>方法可以将GET请求参数转为字典.下面是一个案例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> parse_qs</span><br><span class="line">query = <span class="string">'name=dingzhi&amp;age=20'</span></span><br><span class="line">print(parse_qs(query))</span><br><span class="line"><span class="comment">#运行结果：&#123;'name':['dingzhi'],'age':['20']&#125;</span></span><br></pre></td></tr></table></figure>

<h4 id="parse-qsl-——实现GET请求参数到元组列表的转化"><a href="#parse-qsl-——实现GET请求参数到元组列表的转化" class="headerlink" title="parse_qsl()——实现GET请求参数到元组列表的转化"></a><code>parse_qsl()</code>——实现GET请求参数到元组列表的转化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> parse_qsl</span><br><span class="line">query = <span class="string">'name=dingzhi&amp;age=20'</span></span><br><span class="line">print(parse_qsl(query))</span><br><span class="line"><span class="comment">#运行结果：[&#123;('name','dingzhi'),('age','20')]</span></span><br></pre></td></tr></table></figure>

<h4 id="quote-——实现内容到URL编码的转化"><a href="#quote-——实现内容到URL编码的转化" class="headerlink" title="quote()——实现内容到URL编码的转化"></a><code>quote()</code>——实现内容到URL编码的转化</h4><p>可以将内容转化为URL编码的格式，当URL带有中文参数时，有时可能会导致乱码问题，可以利用这个方法将中文字符转换为URL编码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> quote</span><br><span class="line">keyword = <span class="string">'浙江大学'</span></span><br><span class="line">url = <span class="string">'https://www.baidu.com/s?wd='</span> + quote(keyword)</span><br><span class="line">print(url)</span><br></pre></td></tr></table></figure>

<h4 id="unquote-——实现URL编码到内容的转化"><a href="#unquote-——实现URL编码到内容的转化" class="headerlink" title="unquote()——实现URL编码到内容的转化"></a><code>unquote()</code>——实现URL编码到内容的转化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> unquote</span><br><span class="line">url = <span class="string">'https://search.bilibili.com/all?keyword=%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6&amp;from_source=banner_search'</span></span><br><span class="line">print(unquote(url))</span><br></pre></td></tr></table></figure>

<h3 id="robotparser模块——分析Robots协议"><a href="#robotparser模块——分析Robots协议" class="headerlink" title="robotparser模块——分析Robots协议"></a><code>robotparser</code>模块——分析Robots协议</h3><h4 id="Robots协议概述"><a href="#Robots协议概述" class="headerlink" title="Robots协议概述"></a>Robots协议概述</h4><p>robots是网站跟爬虫间的协议，用简单直接的txt格式文本方式告诉对应的爬虫被允许的权限，也就是说robots.txt是搜索引擎中访问网站的时候要查看的第一个文件。当一个搜索蜘蛛访问一个站点时，它会首先检查该站点根目录下是否存在robots.txt，如果存在，搜索机器人就会按照该文件中的内容来确定访问的范围；如果该文件不存在，所有的搜索蜘蛛将能够访问网站上所有没有被口令保护的页面。</p>
<h5 id="robots-txt文件的格式"><a href="#robots-txt文件的格式" class="headerlink" title="robots.txt文件的格式"></a>robots.txt文件的格式</h5><p>“robots.txt”文件包含一条或更多的记录，这些记录通过空行分开（以CR,CR/NL, or NL作为结束符），每一条记录的格式如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;&lt;field&gt;:&lt;optionalspace&gt;&lt;value&gt;&lt;optionalspace&gt;&quot;。</span><br></pre></td></tr></table></figure>

<p>在该文件中可以使用#进行注解，具体使用方法和UNIX中的惯例一样。该文件中的记录通常以一行或多行User-agent开始，后面加上若干Disallow行,详细情况如下：</p>
<p>User-agent:该项的值用于描述搜索引擎robot的名字，在”robots.txt”文件中，如果有多条User-agent记录说明有多个robot会受到该协议的限制，对该文件来说，至少要有一条User-agent记录。如果该项的值设为<em>，则该协议对任何机器人均有效，在”robots.txt”文件中，”User-agent:</em>“这样的记录只能有一条。</p>
<p>Disallow:该项的值用于描述不希望被访问到的一个URL，这个URL可以是一条完整的路径，也可以是部分的，任何以Disallow开头的URL均不会被robot访问到。例如”Disallow:/help”对/help.html 和/help/index.html都不允许搜索引擎访问，而”Disallow:/help/“则允许robot访问/help.html，而不能访问/help/index.html。任何一条Disallow记录为空，说明该网站的所有部分都允许被访问，在”/robots.txt”文件中，至少要有一条Disallow记录。如果”/robots.txt”是一个空文件，则对于所有的搜索引擎robot，该网站都是开放的。</p>
<p>Allow:该项的值用于描述希望被访问的一组URL，与Disallow项相似，这个值可以是一条完整的路径，也可以是路径的前缀，以Allow项的值开头的URL是允许robot访问的。例如”Allow:/hibaidu”允许robot访问/hibaidu.htm、/hibaiducom.html、/hibaidu/com.html。一个网站的所有URL默认是Allow的，所以Allow通常与Disallow搭配使用，实现允许访问一部分网页同时禁止访问其它所有URL的功能。</p>
<p>需要特别注意的是Disallow与Allow行的顺序是有意义的，robot会根据第一个匹配成功的Allow或Disallow行确定是否访问某个URL。</p>
<p>使用”*”和”$”：</p>
<p>robots支持使用通配符”*”和”$”来模糊匹配url：</p>
<p>“$” 匹配行结束符。</p>
<p>“*” 匹配0或多个任意字符。</p>
<h6 id="robots-txt语法"><a href="#robots-txt语法" class="headerlink" title="robots.txt语法"></a>robots.txt语法</h6><ul>
<li><p>允许所有SE收录本站：robots.txt为空就可以，什么都不要写。</p>
</li>
<li><p>禁止所有SE收录网站的某些目录：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">User-agent: *</span><br><span class="line">Disallow: /目录名1/</span><br><span class="line">Disallow: /目录名2/</span><br><span class="line">Disallow: /目录名3/</span><br></pre></td></tr></table></figure>
</li>
<li><p>禁止某个SE收录本站，例如禁止百度：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">User-agent: Baiduspider</span><br><span class="line">Disallow: /</span><br></pre></td></tr></table></figure>
</li>
<li><p>禁止所有SE收录本站：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">User-agent: *</span><br><span class="line">Disallow: /</span><br></pre></td></tr></table></figure>
</li>
<li><p>加入sitemap.xml路径</p>
</li>
</ul>
<h5 id="robotparser"><a href="#robotparser" class="headerlink" title="robotparser"></a><code>robotparser</code></h5><p>该模块提供了一个类<code>RobotFileParser</code>，可以根据某个网站的<code>robots.txt</code>文件判断一个爬去爬虫是否有权限爬取这个网页.这个类使用十分简单，只需要在构造方法里传入<code>robots.txt</code>的链接即可，其声明如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">urllib.robotparser.RobotFileParser(url=<span class="string">''</span>)</span><br></pre></td></tr></table></figure>

<p>当然也可以在声明时不传入而默认为空，最后再使用<code>set_url()</code>方法设置一下即可，下面列出这个类的常用的几个方法：</p>
<ul>
<li><code>set_url()</code>：用来设置<code>robots.txt</code>文件链接</li>
<li><code>read()</code>：读取<code>robots.txt</code>文件并进行分析，注意，这个方法执行一个读取和分析操作，如果不调用这个方法，接下来判断都会为False，所以一定要调用这个方法，这个方法不会返回任何内容，但是执行了读取操作</li>
<li><code>parse()</code>：用来解析<code>robots.txt</code>文件，传入的参数是<code>robots.txt</code>某些行的内容，按照<code>robots.txt</code>的语法规则分析这些内容</li>
<li><code>can_fetch()</code>：该方法传入两个参数，第一个是<code>User-Agent</code>,第二个是要抓取的URL，返回的内容是该搜索引擎是否可以抓取这个URL，返回结果是True或者False.</li>
<li><code>mtime()</code>：返回的是上次抓取和分析<code>robots.txt</code>的时间，这对于长时间分析和抓取的搜索爬虫很有必要，你可能需要定期检查抓取最新的<code>robots.txt</code></li>
<li><code>modified()</code>：对长时间分析和抓取的搜索爬虫很有帮助，将当前时间设置为上次抓取和分析<code>robots.txt</code>的时间.</li>
</ul>
<p>下面来看一个案例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.robotparser <span class="keyword">import</span> RobotFileParser</span><br><span class="line">rp = RobotFileParser()</span><br><span class="line">rp.set_url(<span class="string">'http://www.jianshu.com/robots.txt'</span>)</span><br><span class="line">rp.read()</span><br><span class="line">print(rp.can_fetch(<span class="string">'*'</span>, <span class="string">'http://www.jianshu.com/p/b67554025d7d'</span>))</span><br><span class="line">print(rp.can_fetch(<span class="string">'*'</span>, <span class="string">"http://www.jianshu.com/search?q=python&amp;page=1&amp;type=collections"</span>))</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">运行结果：</span></span><br><span class="line"><span class="string">True</span></span><br><span class="line"><span class="string">False</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>

<p>也可以使用先前的parse方法实现分析，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.robotparser <span class="keyword">import</span> RobotFileParser</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen</span><br><span class="line">rp = RobotFileParser()</span><br><span class="line">rp.parse(urlopen(<span class="string">'http://www.jianshu.com/robots.txt'</span>).read().decode(<span class="string">'utf-8'</span>).split(<span class="string">'\n'</span>))</span><br><span class="line">print(rp.can_fetch(<span class="string">'*'</span>, <span class="string">'http://www.jianshu.com/p/b67554025d7d'</span>))</span><br><span class="line">print(rp.can_fetch(<span class="string">'*'</span>, <span class="string">"http://www.jianshu.com/search?q=python&amp;page=1&amp;type=collections"</span>))</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">运行结果：</span></span><br><span class="line"><span class="string">True</span></span><br><span class="line"><span class="string">False</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>

<blockquote class="pullquote right"><p><strong><font size="6">后记 </font></strong></p>
</blockquote>

<p>在这一节中我们主要探讨了Python爬虫基本库urllib的使用，这个库共有四个模块——request（发送请求并得到响应）、parse（解析处理URL链接）、error（处理异常）、robotparser（分析Robots协议），我们通过一系列案例分析了这四个模块的作用，但是在使用urllib库时，有一些不方便的地方，如使用Cookies时需要利用Opener和Handler来实现，而借助更强大的requests库，我们可以更方便的实现Cookies、登录验证、代理设置等操作，我们将在下次博客中进一步介绍关于requests库的相关知识和使用方式，一起感受它的强大之处.</p>
<p>学习爬虫，首先需要掌握一系列基本的库的使用，在今天的这篇文章里我们将一同学习urllib这个基本的库。</p>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>Ding Zhi</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="http://dingzhi.ga/articles/crawler-1/">http://dingzhi.ga/articles/crawler-1/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Attribution-NonCommercial-NoDerivs 3.0 Unported <a href="https://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" rel="external nofollow">(CC BY-NC-ND 3.0)</a></span>
                    </p>
                
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/Python/"># Python</a>
                    
                        <a href="/tags/网络爬虫/"># 网络爬虫</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/articles/regex-2/">小白的正则表达式学习之路——Python之re库篇</a>
            
            
            <a class="next" rel="next" href="/articles/leetcode-2/">LeetCode刷题（二） | 位运算的骚操作</a>
            
        </section>


    </article>
</div>


        </div>
        <footer id="footer" class="footer">
    <title>打赏</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<!--<script type="text/javascript" src="http://static.tctip.com/tctip-1.0.2.js"></script>-->
<script type="text/javascript" src="http://dingzhi.ga/js/tctip-1.0.3.min.js"></script>
<script>
  new tctip({
    top: '20%',
    button: {
      id: 9,
      type: 'dashang'
    },
    list: [
      {
        type: 'alipay',
        qrImg: 'http://dingzhi.ga/images/alipay.jpg'
      }, {
        type: 'wechat',
        qrImg: 'http://dingzhi.ga/images/wechat.jpg'
      }
    ]
  }).init()
</script>
友情链接<span lang="EN-US"> | <a href="http://fenghe.us/" target="_blank">Fenghe's
Blog |</a>
<a href="http://www.kamzero.cn" target="_blank">Sherry's
Blog |</a>
<span>© Ding Zhi 2019 | All Rights Reserved</span>
</span></footer>
    </div>
</body>
</html>
